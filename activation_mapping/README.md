# Activation Mapping
Creates activation of the network, which shows activated nodes. This demonstrates the area within the gait cycle which is most responsible for the intent recognition.

<img src="https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7083369/9133350/9134897/rouse5-3007455-large.gif" width="480">
 
## How to run
1. run `mask_gen.py`
2. run `mapping.py`

## mask_net.py

In this file, we create two different instances of our original network, modified so that we can glean activation information. The first, "Network" only runs the first convolutional layer so that we can use it to create an activation mask in mask_gen.py. Note that we only use the first conv layer, as using the second one results in figures that are too coarse to be useful. "FigureNetwork" simply returns x, as well as the activations after the first convolutional layer, so we can use it to find maximal activations per channel.

## mask_gen.py

This file uses the first modified network to generate an activation mask. This is done by passing in an input full of zeros, with a single NaN inserted, and seeing how that NaN propagates to the next layer. Thus, we can determine what values in the output of the first layer are influenced by the input. This creates mask.npy, which we have also included, so you don't have to run this code.

## mask.npy

This is the mask generated by mask_gen.py.

## mapping.py

In this file, we create the figure shown in the paper. This is done by passing in an actual input to "FigureNetwork", and then finding a channel-wise maximal activation after the fist convolutional layer. Then, the spatial locations of these maximal activations are propagated back to the input using the mask in mask.npy, where they are summed and normalized to generate the final activation figure.

